{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCK.oss             \u001b[1m\u001b[36mandroid\u001b[m\u001b[m              mypy.ini\n",
      "BUILD.bazel          \u001b[1m\u001b[36maten\u001b[m\u001b[m                 \u001b[1m\u001b[36mmypy_plugins\u001b[m\u001b[m\n",
      "CITATION.cff         aten.bzl             pt_ops.bzl\n",
      "CMakeLists.txt       \u001b[1m\u001b[36mbenchmarks\u001b[m\u001b[m           pt_template_srcs.bzl\n",
      "CODEOWNERS           \u001b[1m\u001b[36mbinaries\u001b[m\u001b[m             pyproject.toml\n",
      "CODE_OF_CONDUCT.md   buckbuild.bzl        pytest.ini\n",
      "CONTRIBUTING.md      build.bzl            requirements.txt\n",
      "Dockerfile           build_variables.bzl  \u001b[1m\u001b[36mscripts\u001b[m\u001b[m\n",
      "GLOSSARY.md          \u001b[1m\u001b[36mc10\u001b[m\u001b[m                  setup.py\n",
      "LICENSE              \u001b[1m\u001b[36mcaffe2\u001b[m\u001b[m               \u001b[1m\u001b[36mtest\u001b[m\u001b[m\n",
      "MANIFEST.in          \u001b[1m\u001b[36mcmake\u001b[m\u001b[m                \u001b[1m\u001b[36mthird_party\u001b[m\u001b[m\n",
      "Makefile             defs.bzl             \u001b[1m\u001b[36mtools\u001b[m\u001b[m\n",
      "NOTICE               docker.Makefile      \u001b[1m\u001b[36mtorch\u001b[m\u001b[m\n",
      "README.md            \u001b[1m\u001b[36mdocs\u001b[m\u001b[m                 \u001b[1m\u001b[36mtorchgen\u001b[m\u001b[m\n",
      "RELEASE.md           \u001b[1m\u001b[36mfunctorch\u001b[m\u001b[m            ubsan.supp\n",
      "SECURITY.md          \u001b[1m\u001b[36mios\u001b[m\u001b[m                  ufunc_defs.bzl\n",
      "WORKSPACE            mypy-strict.ini      version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls \"/Users/ijeonghwan/Dev/github/pytorch/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path = \"/Users/ijeonghwan/Dev/github/pytorch/test\"\n",
    "rst_path = \"/Users/ijeonghwan/Dev/github/pytorch/docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".rst 파일의 개수: 195\n"
     ]
    }
   ],
   "source": [
    "# .rst : reStructuredText, python docs 작성에 사용되는 마크업 구문으로 정의, 도큐먼트 형식으로 로드\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "rst_documents = []\n",
    "for dirpath, dirnames, filenames in os.walk(rst_path):\n",
    "    for file in filenames:\n",
    "        if (file.endswith(\".rst\")):\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(\n",
    "                    dirpath, file), encoding=\"utf-8\")\n",
    "                rst_documents.extend(loader.load())\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print(f\".rst 파일의 개수: {len(rst_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py 파일의 개수: 5286\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language\n",
    "\n",
    "\n",
    "py_documents = []\n",
    "\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    code_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(\n",
    "        language=Language.PYTHON, parser_threshold=30\n",
    "    ),\n",
    ")\n",
    "py_documents.extend(loader.load())\n",
    "\n",
    "print(f\".py 파일의 개수: {len(py_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 .py 파일의 개수: 19563\n",
      "분할된 .rst 파일의 개수: 1084\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "py_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "py_docs = py_splitter.split_documents(py_documents)\n",
    "\n",
    "print(f\"분할된 .py 파일의 개수: {len(py_docs)}\")\n",
    "\n",
    "rst_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "rst_docs = rst_splitter.split_documents(rst_documents)\n",
    "\n",
    "print(f\"분할된 .rst 파일의 개수: {len(rst_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 도큐먼트 개수: 20647\n"
     ]
    }
   ],
   "source": [
    "combined_documents = py_docs + rst_docs\n",
    "print(f\"총 도큐먼트 개수: {len(combined_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# print(f\"[API KEY] : {os.environ['OPENAI_API_KEY']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "# Create a LocalFileStore instance to use local file storage\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# Create a OpenAI embedding model instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\n",
    "\n",
    "# Cache Embedding:\n",
    "# Cache the embedding calculation results using CacheBackedEmbeddings\n",
    "# We can reuse values ​​calculated once without having to calculate the embedding multiple times\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=embeddings, \n",
    "    document_embedding_cache=store, \n",
    "    namespace=embeddings.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Specify a folder name for the faiss index to be stored locally\n",
    "FAISS_DB_INDEX = \"langchain_faiss\"\n",
    "\n",
    "# Create a FAISS DB instance\n",
    "db = FAISS.from_documents(combined_documents, cached_embeddings)\n",
    "\n",
    "# Save the created database instance locally in a specified folder\n",
    "db.save_local(folder_path=FAISS_DB_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no need to redo the cells above\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load vector index\n",
    "db = FAISS.load_local(\n",
    "    FAISS_DB_INDEX, # Directory name of the FAISS index to load\n",
    "    cached_embeddings, # Provides embedding information\n",
    "    allow_dangerous_deserialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a VectorStoreRetriever object based on the current vector store\n",
    "# 2000 x 10\n",
    "faiss_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the bm25Retriever class from the langchain.retrievers module\n",
    "from langchain.retrievers import BM25Retriever\n",
    "\n",
    "# Create a BM25 retriever model instance using the document collection\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    combined_documents # document collection for initialization\n",
    ")\n",
    "\n",
    "# Set the k property of the BM25Retriever instance to 10 and return up to 10 results when retrieved.\n",
    "bm25_retriever.k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "    weights=[0.6, 0.4],\n",
    "    search_type=\"mmr\", # MMR method that improves the diversity of search results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"당신은 20년차 AI 개발자입니다. 당신의 임무는 주어진 질문에 대하여 최대한 문서의 정보를 활용하여 답변하는 것입니다.\n",
    "    문서는 Python 코드에 대한 정보를 담고 있습니다. 따라서 답변을 작성할 때에는 Python 코드에 대한 상세한 code snippet을\n",
    "    포함하여 작성해주세요. 최대한 자세하게 답변하고, 한글로 답변해 주세요. 주어진 문서에서 답변을 찾을 수 없는 경우, \"문서에\n",
    "    답변이 없습니다.\"라고 답변해 주세요. 답변은 출처(source)를 반드시 표기해 주세요.\n",
    "    \n",
    "    #참고문서:\n",
    "    {context}\n",
    "\n",
    "    #질문:\n",
    "    {question}\n",
    "\n",
    "    #답변:\n",
    "\n",
    "    출처:\n",
    "    - source1\n",
    "    - source2\n",
    "    - ...\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"gpt4\",\n",
    "    ollama=ChatOllama(\n",
    "        model=\"llama3-instruct-8b:latest\",\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    ),\n",
    "    gpt3=ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        streaming=True,\n",
    "        callbacks=[StreamCallback()],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch.autograd`는 PyTorch에서 자동 미분을 제공하는 기능으로, 신경망 학습 시에 역전파를 통해 파라미터의 그래디언트를 자동으로 계산해줍니다. 사용자는 `torch.Tensor` 객체의 `.backward()` 메소드를 호출하여 그래디언트를 자동으로 계산할 수 있으며, 이를 통해 모델의 가중치를 업데이트할 수 있습니다.\n",
      "\n",
      "기본적인 사용 방법은 다음과 같습니다:\n",
      "\n",
      "1. 모델의 파라미터를 `requires_grad=True`로 설정하여 해당 텐서에 대한 그래디언트 계산이 필요함을 명시합니다.\n",
      "2. 순전파(forward pass)를 실행하여 출력값을 계산합니다.\n",
      "3. 손실 함수(loss function)를 계산합니다.\n",
      "4. `.backward()`를 호출하여 역전파(backward pass)를 실행하고, 각 파라미터의 그래디언트를 자동으로 계산합니다.\n",
      "5. 그래디언트를 사용하여 모델의 파라미터를 업데이트합니다.\n",
      "\n",
      "아래는 간단한 예제 코드입니다:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "# 입력 텐서 x와 파라미터 w, b를 정의하고, requires_grad=True로 설정하여 그래디언트 계산이 필요함을 명시합니다.\n",
      "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
      "w = torch.tensor([2.0], requires_grad=True)\n",
      "b = torch.tensor([1.0], requires_grad=True)\n",
      "\n",
      "# 순전파: 모델의 예측값 y를 계산합니다.\n",
      "y = w * x + b\n",
      "\n",
      "# 손실 함수: 예측값 y와 실제값과의 차이를 제곱하여 손실을 계산합니다.\n",
      "loss = torch.sum((y - torch.tensor([2.0, 4.0, 6.0])) ** 2)\n",
      "\n",
      "# 역전파: 손실 함수의 그래디언트를 계산합니다.\n",
      "loss.backward()\n",
      "\n",
      "# 그래디언트 출력\n",
      "print(f'x.grad: {x.grad}, w.grad: {w.grad}, b.grad: {b.grad}')\n",
      "\n",
      "# 그래디언트를 사용하여 파라미터를 업데이트할 수 있습니다. (예: 경사하강법)\n",
      "# 예를 들어, w와 b를 업데이트하는 코드는 다음과 같을 수 있습니다.\n",
      "# learning_rate = 0.01\n",
      "# w.data -= learning_rate * w.grad.data\n",
      "# b.data -= learning_rate * b.grad.data\n",
      "```\n",
      "\n",
      "이 예제에서는 간단한 선형 모델 `y = wx + b`를 정의하고, 손실 함수로 예측값과 실제값의 차이의 제곱합을 사용했습니다. `.backward()` 메소드를 호출하여 손실 함수의 그래디언트를 계산하고, 이를 출력하여 확인할 수 있습니다.\n",
      "\n",
      "참고문서에서는 `torch.autograd`의 사용 방법에 대한 구체적인 예제 코드가 제공되지 않았으므로, 위의 예제 코드는 `torch.autograd`의 기본적인 사용 방법을 설명하기 위한 것입니다."
     ]
    }
   ],
   "source": [
    "answer = rag_chain.with_config(configurable={\"llm\": \"gpt4\"}).invoke(\n",
    "    \"torch.autograd 사용 방법을 알려주세요\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch.cuda`는 PyTorch에서 NVIDIA CUDA를 사용하여 GPU 상에서 텐서 연산을 가속화하기 위한 모듈입니다. 이 모듈을 통해 GPU에서의 계산을 관리하고 최적화할 수 있는 다양한 기능과 도구를 제공합니다. 여기에는 GPU 장치 관리, 메모리 관리, 스트림 및 이벤트 관리 등이 포함됩니다.\n",
      "\n",
      "주요 기능들은 다음과 같습니다:\n",
      "\n",
      "- `torch.cuda.is_available()`: CUDA가 사용 가능한지 여부를 반환합니다. 이는 GPU가 시스템에 설치되어 있고 PyTorch가 CUDA를 사용할 수 있는 상태인지 확인하는 데 사용됩니다.\n",
      "- `torch.cuda.device_count()`: 사용 가능한 GPU 장치의 수를 반환합니다.\n",
      "- `torch.cuda.set_device(device)`: 기본 GPU 장치를 설정합니다.\n",
      "- `torch.cuda.current_device()`: 현재 활성화된 GPU 장치의 인덱스를 반환합니다.\n",
      "- `torch.cuda.memory_allocated()`: 현재 할당된 메모리의 양을 반환합니다.\n",
      "- `torch.cuda.memory_reserved()`: 현재 예약된(총 할당 가능한) 메모리의 양을 반환합니다.\n",
      "- `torch.cuda.synchronize()`: 현재 선택된 GPU 장치의 모든 스트림을 동기화합니다.\n",
      "\n",
      "또한, `torch.cuda.Stream`을 사용하여 비동기 연산을 관리할 수 있으며, `torch.cuda.Event`를 통해 연산의 시작과 끝을 표시하여 성능 측정에 사용할 수 있습니다.\n",
      "\n",
      "CUDA 메모리 관리에 관련된 더 세부적인 기능으로는 메모리 할당, 해제 및 최적화를 위한 도구들이 있습니다. 예를 들어, `torch.cuda.empty_cache()`를 호출하여 불필요한 메모리를 해제하고 GPU 메모리 사용량을 최적화할 수 있습니다.\n",
      "\n",
      "이러한 기능들은 GPU를 사용하여 딥러닝 모델을 훈련시키거나, 대규모 텐서 연산을 수행할 때 중요한 역할을 합니다. PyTorch의 `torch.cuda` 모듈을 통해 개발자는 GPU 리소스를 효율적으로 관리하고, 연산 속도를 향상시킬 수 있습니다.\n",
      "\n",
      "참고문서:\n",
      "- `/Users/ijeonghwan/Dev/github/pytorch/docs/source/cuda.rst`"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.with_config(configurable={\"llm\": \"gpt4\"}).invoke(\n",
    "    \"torch.cuda 에 대해서 알려주세요\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch.optim`은 PyTorch에서 모델의 파라미터를 최적화하기 위해 사용되는 모듈입니다. 이 모듈은 다양한 최적화 알고리즘을 제공하여, 모델 학습 시 그래디언트(기울기)를 기반으로 파라미터를 업데이트하는 과정을 쉽게 만들어 줍니다.\n",
      "\n",
      "`torch.optim`을 사용하기 위해서는 먼저 최적화할 모델의 파라미터와 함께 최적화 알고리즘을 선택해야 합니다. 그 후, `.step()` 메소드를 호출하여 파라미터를 업데이트합니다. 이 과정은 일반적으로 손실 함수의 그래디언트를 계산한 후에 이루어집니다.\n",
      "\n",
      "예를 들어, SGD(Stochastic Gradient Descent) 최적화 알고리즘을 사용하는 경우 다음과 같이 최적화 객체를 생성할 수 있습니다:\n",
      "\n",
      "```python\n",
      "import torch.optim as optim\n",
      "\n",
      "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
      "```\n",
      "\n",
      "여기서 `model.parameters()`는 최적화할 모델의 파라미터를 나타내고, `lr`은 학습률(learning rate)을, `momentum`은 모멘텀 값을 의미합니다.\n",
      "\n",
      "또한, `torch.optim`은 파라미터 그룹별로 다른 최적화 옵션을 지정할 수 있도록 지원합니다. 예를 들어, 모델의 일부 파라미터에는 더 높은 학습률을 사용하고 싶을 때 다음과 같이 설정할 수 있습니다:\n",
      "\n",
      "```python\n",
      "optimizer = optim.SGD([\n",
      "    {'params': model.base.parameters(), 'lr': 1e-2},\n",
      "    {'params': model.classifier.parameters()}\n",
      "], lr=1e-3, momentum=0.9)\n",
      "```\n",
      "\n",
      "이 코드는 `model.base`의 파라미터에는 `1e-2`의 학습률을, `model.classifier`의 파라미터에는 기본 학습률인 `1e-3`을 사용하도록 설정합니다. 모든 파라미터 그룹에 대해 `momentum=0.9`가 적용됩니다.\n",
      "\n",
      "출처: `/Users/ijeonghwan/Dev/github/pytorch/docs/source/optim.rst` 문서에서 정보를 바탕으로 작성되었습니다."
     ]
    }
   ],
   "source": [
    "answer = rag_chain.with_config(configurable={\"llm\": \"ollama\"}).invoke(\n",
    "    \"torch optim에 대해서 알려주세요\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('pytorch-analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12b3d405831e4f113b86698532c94a35d757aa3cdb819105bb5ba0568c2dc78d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
